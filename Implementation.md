My implementation of the tokenizer class is as follows:

Two methods provide token(generic) identification, i.e tokens without whitespace; getStartOfNextToken and getEndOfNextToken. Both methods return an integer index. Preceding whitespace if cleared by getStart... until the first non-whitespace character is found. If the first non-whitespace character in the token is an uppercase/lower case letter or integer, then look at the next character until a different type of character is found. The same logic is applied for special symbols, but only if including the next character in the token is also a special symbol. After the end of a token is discovered, the start of the next token is set to the index of the last token + 1. Before the next token is found, the currently discovered token(generic) is passed if it contains whitespace(which would mean it is entirely whitespace). If it is not whitespace, the token is classified(non-generic) by checking against sets of the reserved words, special symbols, a RegEx for integer, and a RegEx for identifiers. If the token is not classified, an error message is returned and the program haults. If no error is returned, then the program proceeds to tokenize the rest of the file by the same process.

I felt it was easier to tokenize the entire file upfront,storing the tokens in a list, with a tokenize method, and report an error if one occurs. I felt that only tokenizing the file as a user calls the nextToken() and currentToken() was sloppy and left room for more errors. Those methods are included in the Tokenizer API, but they only reference items in the tokenizer list that was created rather than doing the "heavy lifting" of actually tokenizing the file. I also thought it may be useful for later project that use the Tokenizer class to be able to access all of the tokens in a sequencial list.

